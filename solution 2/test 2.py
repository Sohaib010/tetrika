import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import unquote

# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±—É–∫–≤ —Ä—É—Å—Å–∫–æ–≥–æ –∞–ª—Ñ–∞–≤–∏—Ç–∞
#letters = ['–ê', '–ë', '–í', '–ì', '–î', '–ï', '–Å', '–ñ', '–ó', '–ò', '–ô', '–ö', '–õ', '–ú', '–ù', '–û', '–ü', '–†', '–°', '–¢', '–£', '–§', '–•', '–¶', '–ß', '–®', '–©', '–≠', '–Æ', '–Ø']

letters = [chr(i) for i in range(ord('–ê'), ord('–Ø') + 1)]
letters.insert(6, '–Å')


# –û—Ç–∫—Ä—ã–≤–∞–µ–º CSV-—Ñ–∞–π–ª
with open('beasts.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['–ë—É–∫–≤–∞', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∂–∏–≤–æ—Ç–Ω—ã—Ö'])
    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–π –±—É–∫–≤–µ
    for letter in letters:
        url = f'https://ru.wikipedia.org/w/index.php?title=–ö–∞—Ç–µ–≥–æ—Ä–∏—è:–ñ–∏–≤–æ—Ç–Ω—ã–µ_–ø–æ_–∞–ª—Ñ–∞–≤–∏—Ç—É&from={letter}'
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∂–∏–≤–æ—Ç–Ω—ã—Ö
        links = soup.find_all('a', href=True)
        animals = [
            link for link in links
            if link['href'].startswith('/wiki/')
            and unquote(link['href'][6:])[0].upper() == letter
        ]

        print(f'\nüî† –ë—É–∫–≤–∞: {letter} ‚Äî –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(animals)}')
        
        for link in animals:
            if letter == '–Å':
                
            
              print(f'‚Äî {link.text.strip()}')

        writer.writerow([letter, len(animals)])

print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ 'beasts.csv'")
